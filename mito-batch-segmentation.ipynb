{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Segmentation of mitochondria in WebKnossos datasets with Empanada**\n",
    "---\n",
    "Perform batch segmentation of mitochondria using MitoNet for a full data set\n",
    "- Import dataset in chunks from WebKnossos\n",
    "- Perform 3D inference of mitochondria using `Empanada`\n",
    "- Upload resulting segmentations as a segmentation layer to WebKnossos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data import\n",
    "import webknossos as wk\n",
    "import numpy as np\n",
    "import zarr\n",
    "\n",
    "# Napari\n",
    "import napari\n",
    "from empanada_napari._volume_inference import volume_inference_widget\n",
    "import empanada\n",
    "\n",
    "# Empanada\n",
    "import os\n",
    "import torch\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from skimage import io\n",
    "\n",
    "import torch.multiprocessing as mp\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "from empanada.data import VolumeDataset\n",
    "from empanada.inference.engines import PanopticDeepLabRenderEngine3d\n",
    "from empanada.inference import filters\n",
    "from empanada.config_loaders import load_config\n",
    "from empanada.inference.patterns import *\n",
    "\n",
    "# Analysis\n",
    "# import seaborn as sns\n",
    "import pandas as pd\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_wk_dataset_remote(TOKEN, \n",
    "                             url, \n",
    "                             dataset_name, \n",
    "                             organization_id, \n",
    "                             MAG,\n",
    "                             layer=\"color\") -> None:\n",
    "    # use the context to get acces to your group\n",
    "    with wk.webknossos_context(\n",
    "        token=TOKEN,\n",
    "        url=url\n",
    "    ):\n",
    "\n",
    "    # open remote dataset with dataset name, organization id and WebKnossos url \n",
    "        dataset = wk.Dataset.open_remote(\n",
    "                dataset_name_or_url = dataset_name,\n",
    "                organization_id = organization_id,\n",
    "                webknossos_url = url)\n",
    "        voxel_size = dataset.voxel_size\n",
    "\n",
    "        EM = dataset.get_layer(layer) # Layer\n",
    "        mag_view = EM.get_mag(MAG) # MagView\n",
    "        \n",
    "    # return data, voxel size\n",
    "    return mag_view, voxel_size\n",
    "\n",
    "def import_wk_dataset_local(dir_path,\n",
    "                            layer=\"color\") -> None:\n",
    "    # open local dataset in given directory \n",
    "    dataset = wk.Dataset.open(\n",
    "        dataset_path = dir_path)\n",
    "    voxel_size = dataset.voxel_size\n",
    "\n",
    "    EM = dataset.get_layer(layer) # Layer\n",
    "    mag_view = EM.get_mag(MAG) # MagView\n",
    "        \n",
    "    # return data, voxel size\n",
    "    return dataset, mag_view, voxel_size\n",
    "\n",
    "def get_data_from_bbox(mag_view, bbox) -> None:\n",
    "    # Generate view from bounding box and read the zarr \n",
    "    view = mag_view.get_view(absolute_offset=bbox.topleft, \n",
    "                             size=bbox.size) # \"absolute_offset\" and \"size\" are in Mag(1)!\n",
    "    data = view.read() # reads the actual data\n",
    "\n",
    "    #return data chunk\n",
    "    return data\n",
    "\n",
    "def inference_3d(config, volume_data, mode='stack', qlen=3, nmax=20000, seg_thr=0.3, nms_thr=0.1, nms_kernel=3, \n",
    "                iou_thr=0.25, ioa_thr=0.25, pixel_vote_thr=2, cluster_io_thr=0.75, min_size=200, \n",
    "                min_span=2, downsample_f=1, one_view=True, fine_boundaries=False, use_cpu=True):\n",
    "                \n",
    "    # read the model config file\n",
    "    config = load_config(config)\n",
    "\n",
    "    # set device and determine model to load\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() and not use_cpu else \"cpu\")\n",
    "    use_quantized = str(device) == 'cpu' and config.get('model_quantized') is not None\n",
    "    model_key = 'model_quantized' if use_quantized  else 'model'\n",
    "    \n",
    "    if os.path.isfile(config[model_key]):\n",
    "        model = torch.jit.load(config[model_key])\n",
    "    else:\n",
    "        model = torch.hub.load_state_dict_from_url(config[model_key])\n",
    "\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # data\n",
    "    volume = np.squeeze(volume_data)\n",
    "    shape = volume.shape\n",
    "\n",
    "    if mode == 'orthoplane':\n",
    "        axes = {'xy': 0, 'xz': 1, 'yz': 2} # x, y, z\n",
    "    else:\n",
    "        axes = {'yz': 2}\n",
    "\n",
    "    eval_tfs = A.Compose([\n",
    "        A.Normalize(**config['norms']),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "\n",
    "    trackers = {}\n",
    "    class_labels = list(config['class_names'].keys())\n",
    "    thing_list = config['thing_list']\n",
    "    label_divisor = nmax\n",
    "\n",
    "    # create a separate tracker for\n",
    "    # each prediction axis and each segmentation class\n",
    "    trackers = create_axis_trackers(axes, class_labels, label_divisor, shape)\n",
    "\n",
    "    for axis_name, axis in axes.items():\n",
    "        print(f'Predicting {axis_name} stack')\n",
    "        stack = None\n",
    "\n",
    "        # create the inference engine\n",
    "        inference_engine = PanopticDeepLabRenderEngine3d(\n",
    "            model, thing_list=thing_list,\n",
    "            median_kernel_size=qlen,\n",
    "            label_divisor=label_divisor,\n",
    "            nms_threshold=nms_thr,\n",
    "            nms_kernel=nms_kernel,\n",
    "            confidence_thr=seg_thr,\n",
    "            padding_factor=config['padding_factor'],\n",
    "            coarse_boundaries=not fine_boundaries\n",
    "        )\n",
    "\n",
    "        # create a separate matcher for each thing class\n",
    "        matchers = create_matchers(thing_list, label_divisor, iou_thr, ioa_thr)\n",
    "\n",
    "        # setup matcher for multiprocessing\n",
    "        queue = mp.Queue()\n",
    "        rle_stack = []\n",
    "        matcher_out, matcher_in = mp.Pipe()\n",
    "        matcher_args = (\n",
    "            matchers, queue, rle_stack, matcher_in,\n",
    "            class_labels, label_divisor, thing_list\n",
    "        )\n",
    "        matcher_proc = mp.Process(target=forward_matching, args=matcher_args)\n",
    "        matcher_proc.start()\n",
    "\n",
    "        # make axis-specific dataset\n",
    "        dataset = VolumeDataset(volume, axis, eval_tfs, scale=downsample_f)\n",
    "\n",
    "        num_workers = 1\n",
    "        dataloader = DataLoader(\n",
    "            dataset, batch_size=1, shuffle=False,\n",
    "            pin_memory=(device == 'gpu'), drop_last=False,\n",
    "            num_workers=num_workers\n",
    "        )\n",
    "\n",
    "        for batch in tqdm(dataloader, total=len(dataloader)):\n",
    "            image = batch['image']\n",
    "            size = batch['size']\n",
    "\n",
    "            # pads and crops image in the engine\n",
    "            # upsample output by same factor as downsampled input\n",
    "            pan_seg = inference_engine(image, size, upsampling=downsample_f)\n",
    "\n",
    "            if pan_seg is None:\n",
    "                queue.put(None)\n",
    "                continue\n",
    "            else:\n",
    "                pan_seg = pan_seg.squeeze().cpu().numpy()\n",
    "                queue.put(pan_seg)\n",
    "\n",
    "        final_segs = inference_engine.end(downsample_f)\n",
    "        if final_segs:\n",
    "            for i, pan_seg in enumerate(final_segs):\n",
    "                pan_seg = pan_seg.squeeze().cpu().numpy()\n",
    "                queue.put(pan_seg)\n",
    "\n",
    "        # finish and close forward matching process\n",
    "        queue.put('finish')\n",
    "        rle_stack = matcher_out.recv()[0]\n",
    "        matcher_proc.join()\n",
    "\n",
    "        print(f'Propagating labels backward through the stack...')\n",
    "        for index,rle_seg in tqdm(backward_matching(rle_stack, matchers, shape[axis]), total=shape[axis]):\n",
    "            update_trackers(rle_seg, index, trackers[axis_name])\n",
    "\n",
    "        finish_tracking(trackers[axis_name])\n",
    "        for tracker in trackers[axis_name]:\n",
    "            filters.remove_small_objects(tracker, min_size=min_size)\n",
    "            filters.remove_pancakes(tracker, min_span=min_span)\n",
    "\n",
    "    # create the final instance segmentations\n",
    "    for class_id, class_name in config['class_names'].items():\n",
    "        print(f'Creating consensus segmentation for class {class_name}...')\n",
    "        class_trackers = get_axis_trackers_by_class(trackers, class_id)\n",
    "\n",
    "        # merge instances from orthoplane inference if applicable\n",
    "        if mode == 'orthoplane':\n",
    "            if class_id in thing_list:\n",
    "                consensus_tracker = create_instance_consensus(\n",
    "                    class_trackers, pixel_vote_thr, cluster_iou_thr, one_view\n",
    "                )\n",
    "                filters.remove_small_objects(consensus_tracker, min_size=min_size)\n",
    "                filters.remove_pancakes(consensus_tracker, min_span=min_span)\n",
    "            else:\n",
    "                consensus_tracker = create_semantic_consensus(class_trackers, pixel_vote_thr)\n",
    "        else:\n",
    "            consensus_tracker = class_trackers[0]\n",
    "\n",
    "        dtype = np.uint32 if class_id in thing_list else np.uint8\n",
    "\n",
    "        # decode and fill the instances\n",
    "        consensus_vol = np.zeros(shape, dtype=dtype)\n",
    "        fill_volume(consensus_vol, consensus_tracker.instances)\n",
    "\n",
    "    print('Finished!')\n",
    "    return consensus_vol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.1 Configure dataset parameters**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN = \"A_KRh0eDGiIuQMTL1EKxWg\" # Generate from https://webknossos.tnw.tudelft.nl/auth/token. Don't share this token online or in publications!!!\n",
    "url = \"https://webknossos.tnw.tudelft.nl\" # \"https://webknossos.tnw.tudelft.nl\" \n",
    "dataset_name = \"20230105_NK_01_with_predictions\" # Dataset name as in WebKnossos\n",
    "organization_id = \"hoogenboom-group\" # \"hoogenboom-group\"\n",
    "dir_path = f\"Z:/webknossos/binaryData/hoogenboom-group/{dataset_name}\"\n",
    "config = os.path.abspath(\"configs/MitoNet_v1.yaml\") # MitoNet model configuration file\n",
    "layer = \"KO\"\n",
    "\n",
    "mag_x, mag_y, mag_z = 4, 4, 1 # Magnification level (x, y, z) in WebKnossos to be used for segmentation. Default is (4, 4, 1)\n",
    "bbox_size = 2048 # pixels, at desired zoom level. Default is (4, 4, 1)\n",
    "MAG = wk.Mag(f\"{mag_x}-{mag_y}-{mag_z}\") # Set magnification for WebKnossos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.2 (Remote) import data from WebKnossos**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import full data set at required zoom level (remote)\n",
    "# mag_view, voxelsize = import_wk_dataset_local(TOKEN, \n",
    "#                                                url, \n",
    "#                                                dataset_name, \n",
    "#                                                organization_id, \n",
    "#                                                MAG)\n",
    "\n",
    "# Import full data set at required zoom level (local)\n",
    "dataset, mag_view, voxelsize = import_wk_dataset_local(dir_path,\n",
    "                                                       layer=layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Infer data set dimensions (in desired mag)\n",
    "dim = mag_view.bounding_box.in_mag(MAG).bottomright\n",
    "\n",
    "# Determine number of chunks to split data into in all dimensions\n",
    "chunks_x, chunks_y, chunks_z = math.ceil(dim.x / bbox_size), math.ceil(dim.y / bbox_size), math.ceil(dim.z / 256)\n",
    "\n",
    "# Determine z size of bbox (x an y sizes are defined by bbox_size)\n",
    "size_z = min(dim.z, 256)\n",
    "\n",
    "# From # chunks, define bboxes to be used\n",
    "# Loop over x, y, z chunk indices, define bbox on multiples of bbox_size\n",
    "bboxes = []\n",
    "for i in range(chunks_x):\n",
    "    for j in range(chunks_y):\n",
    "        if bbox_size*(i+1) >= dim.x: # check if bbox is larger than max x and adjust bbox dimension \n",
    "            bbox_size_x = dim.x - bbox_size*i\n",
    "        else: # not exceeding stack dimensions, use regular bbox_size\n",
    "            bbox_size_x = bbox_size\n",
    "        \n",
    "        if bbox_size*(j+1) >= dim.y: # check if bbox is larger than max y and adjust bbox dimension \n",
    "            bbox_size_y = dim.y - bbox_size*j\n",
    "        else: # not exceeding stack dimensions, use regular bbox_size\n",
    "            bbox_size_y =  bbox_size\n",
    "        \n",
    "        for k in range(chunks_z):\n",
    "            # Generate bbox\n",
    "            bbox = wk.BoundingBox(topleft=(bbox_size*i, bbox_size*j, 256*k),\n",
    "                                  size=(bbox_size_x, bbox_size_y, size_z))\\\n",
    "                                    .from_mag_to_mag1(from_mag=MAG)        \n",
    "            bboxes.append(bbox)                                                       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 **Import data into napari and run Empanada (MitoNet)**\n",
    "---\n",
    "Reads data into Napari and performs the segmentation with Empanada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make new \"segmentation\" layer\n",
    "segmentation_layer = dataset.add_layer(\n",
    "        \"KO-mito-segmentation-MitoNet\", \n",
    "        wk.SEGMENTATION_CATEGORY,\n",
    "        compressed=True,\n",
    "        largest_segment_id=None)\n",
    "mag = segmentation_layer.add_mag(MAG, compress=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dab2ac5a6f4f45f781a511262cdaa581",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ajkievits\\Miniconda3\\envs\\mito-seg\\lib\\site-packages\\torch\\serialization.py:799: UserWarning: 'torch.load' received a zip file that looks like a TorchScript archive dispatching to 'torch.jit.load' (call 'torch.jit.load' directly to silence this warning)\n",
      "  warnings.warn(\"'torch.load' received a zip file that looks like a TorchScript archive\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting yz stack\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14a2f0d478774e3d91e71733ab3a6563",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Propagating labels backward through the stack...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ef4bd2960ba4644950d23175f8e6fb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating consensus segmentation for class mito...\n",
      "Finished!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00a8ee05313e4cb3b6935cb402d2afb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loop over bboxes, read data, do 3D segmentation, upload\n",
    "for bbox in tqdm(bboxes, total=len(bboxes)):\n",
    "        \n",
    "        # Get data from bbox\n",
    "        data = get_data_from_bbox(mag_view, bbox)\n",
    "\n",
    "        # Run 3d inference\n",
    "        mito_labels = inference_3d(config, data, mode='stack', qlen=1, nmax=20000, seg_thr=0.5, nms_thr=0.1, nms_kernel=3, \n",
    "                                iou_thr=0.25, ioa_thr=0.25, pixel_vote_thr=2, cluster_io_thr=0.75, min_size=200, \n",
    "                                min_span=1, downsample_f=1, one_view=True, fine_boundaries=False, use_cpu=True)\n",
    "\n",
    "                                # TO DO \n",
    "                                # Figure out why prediction has to be run on \"yz\" stack instead of \"xy\". Also in napari empanada the prediction has to be run on the \n",
    "                                # \"yz\" stack and not \"xy\". Has this to do with the zarr format from WebKnossos? Interestingly, napari shows the data stack \n",
    "                        \n",
    "        # Add \"Mag\" and write segmentation data to \"Mag\"\n",
    "        mag.write(data=mito_labels,\n",
    "                  absolute_offset=bbox.topleft)\n",
    "        segmentation_layer.refresh_largest_segment_id()\n",
    "\n",
    "# Create other zoom levels\n",
    "print(\"Downsampling resulting segmentations...\")\n",
    "segmentation_layer.downsample(sampling_mode=\"constant_z\")\n",
    "print(\"Downsampling completed...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "napari-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
